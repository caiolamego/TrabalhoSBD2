{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2003853d",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANT: How to Use This Notebook\n",
    "\n",
    "### üîß First Time Setup (or after dependency issues)\n",
    "\n",
    "1. **Execute Cell 5** (Install Dependencies) - This will install numpy 1.x, pandas 2.0.3, psycopg2-binary\n",
    "2. **RESTART THE KERNEL** ‚Üê CRITICAL! (Menu ‚Üí Kernel ‚Üí Restart Kernel)\n",
    "3. **Skip cells 1-6**, start from Cell 7 (## 1. Environment Setup)\n",
    "4. Execute cells 7-31 in sequence\n",
    "\n",
    "### üöÄ Normal Usage (after dependencies are installed)\n",
    "\n",
    "**Option A - Clean Start:**\n",
    "- Restart kernel\n",
    "- Execute cells 7-31 in sequence\n",
    "\n",
    "**Option B - Continue Running Kernel:**\n",
    "- Execute cells in sequence from 7 onwards\n",
    "- If you get numpy/pandas errors, execute cell 20 before cell 21\n",
    "\n",
    "### üêõ Troubleshooting\n",
    "\n",
    "**Error: \"numpy.dtype size changed, may indicate binary incompatibility\"**\n",
    "- Cause: numpy 2.x is installed (incompatible with pandas 2.0.3)\n",
    "- Solution: Run Cell 5, then RESTART KERNEL, skip to Cell 7\n",
    "\n",
    "**Error: \"ModuleNotFoundError: No module named 'psycopg2._psycopg'\"**\n",
    "- Cause: Loading psycopg2 from wrong Python version\n",
    "- Solution: Run Cell 20 (clears cache), then run Cell 21\n",
    "\n",
    "See `TROUBLESHOOTING_NUMPY.md` for detailed explanation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe612434",
   "metadata": {},
   "source": [
    "## Diagnostic - Check Python Version\n",
    "\n",
    "Verify which Python version the kernel is actually using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535cbc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /usr/bin/python3.11\n",
      "Python version: 3.11.12 (main, Apr  9 2025, 08:55:55) [GCC 13.3.0]\n",
      "Python version info: sys.version_info(major=3, minor=11, micro=12, releaselevel='final', serial=0)\n",
      "\n",
      "First 5 sys.path entries:\n",
      "  0: /home/davi/.local/lib/python3.11/site-packages\n",
      "  1: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/plugins\n",
      "  2: /tmp/spark-2de69f26-d690-439f-bf55-45a29dde18f2/userFiles-7ede4014-2717-423a-80fd-150467d7b67a\n",
      "  3: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/helpers\n",
      "  4: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/spark_config\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")\n",
    "print(f\"\\nFirst 5 sys.path entries:\")\n",
    "for i, path in enumerate(sys.path[:5]):\n",
    "    print(f\"  {i}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4620b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /usr/bin/python3.11\n",
      "Version: 3.11.12 (main, Apr  9 2025, 08:55:55) [GCC 13.3.0]\n",
      "\n",
      "Installed packages:\n",
      "  numpy==1.26.4\n",
      "  pandas==2.0.3\n",
      "  psycopg2-binary==2.9.9\n",
      "\n",
      "Actual loaded versions:\n",
      "  ‚úÖ numpy: 1.26.4\n",
      "     Location: /home/davi/.local/lib/python3.11/site-packages/numpy/__init__.py\n",
      "  ‚úÖ pandas: 2.0.3\n",
      "  ‚úÖ psycopg2: 2.9.9 (dt dec pq3 ext lo64)\n",
      "\n",
      "============================================================\n",
      "COMPATIBILITY CHECK:\n",
      "============================================================\n",
      "‚úÖ COMPATIBLE: numpy 1.x + pandas 2.0.x is the correct combination\n",
      "============================================================\n",
      "Installed packages:\n",
      "  numpy==1.26.4\n",
      "  pandas==2.0.3\n",
      "  psycopg2-binary==2.9.9\n",
      "\n",
      "Actual loaded versions:\n",
      "  ‚úÖ numpy: 1.26.4\n",
      "     Location: /home/davi/.local/lib/python3.11/site-packages/numpy/__init__.py\n",
      "  ‚úÖ pandas: 2.0.3\n",
      "  ‚úÖ psycopg2: 2.9.9 (dt dec pq3 ext lo64)\n",
      "\n",
      "============================================================\n",
      "COMPATIBILITY CHECK:\n",
      "============================================================\n",
      "‚úÖ COMPATIBLE: numpy 1.x + pandas 2.0.x is the correct combination\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verificar vers√µes instaladas e compatibilidade\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(f\"Python: {sys.executable}\")\n",
    "print(f\"Version: {sys.version}\\n\")\n",
    "\n",
    "# Check installed packages\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"list\", \"--format=freeze\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"Installed packages:\")\n",
    "for line in result.stdout.split('\\n'):\n",
    "    if any(pkg in line.lower() for pkg in ['numpy', 'pandas', 'psycopg']):\n",
    "        print(f\"  {line}\")\n",
    "\n",
    "# Try importing to see actual versions\n",
    "print(\"\\nActual loaded versions:\")\n",
    "try:\n",
    "    import numpy\n",
    "    print(f\"  ‚úÖ numpy: {numpy.__version__}\")\n",
    "    print(f\"     Location: {numpy.__file__}\")\n",
    "    \n",
    "    # Check numpy compatibility\n",
    "    numpy_major = int(numpy.__version__.split('.')[0])\n",
    "    if numpy_major >= 2:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: numpy 2.x is INCOMPATIBLE with pandas 2.0.3!\")\n",
    "        print(f\"     You must install numpy 1.x (e.g., 1.26.4)\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå numpy: ERROR - {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    print(f\"  ‚úÖ pandas: {pandas.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå pandas: ERROR - {e}\")\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "    print(f\"  ‚úÖ psycopg2: {psycopg2.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå psycopg2: ERROR - {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPATIBILITY CHECK:\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    import numpy\n",
    "    import pandas\n",
    "    numpy_ver = tuple(map(int, numpy.__version__.split('.')[:2]))\n",
    "    pandas_ver = tuple(map(int, pandas.__version__.split('.')[:2]))\n",
    "    \n",
    "    if numpy_ver[0] >= 2 and pandas_ver == (2, 0):\n",
    "        print(\"‚ùå INCOMPATIBLE: numpy 2.x + pandas 2.0.x will cause binary errors!\")\n",
    "        print(\"   Solution: Install numpy 1.x with: pip install 'numpy<2.0'\")\n",
    "    elif numpy_ver[0] == 1 and pandas_ver == (2, 0):\n",
    "        print(\"‚úÖ COMPATIBLE: numpy 1.x + pandas 2.0.x is the correct combination\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Unknown combination: numpy {numpy.__version__} + pandas {pandas.__version__}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Could not check compatibility\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2456773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIXING DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "üîß Step 1: Removing broken installations...\n",
      "üóëÔ∏è  Uninstalling psycopg2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping psycopg2 as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ psycopg2 uninstalled\n",
      "üóëÔ∏è  Uninstalling psycopg2-binary...\n",
      "Found existing installation: psycopg2-binary 2.9.9\n",
      "Uninstalling psycopg2-binary-2.9.9:\n",
      "  Successfully uninstalled psycopg2-binary-2.9.9\n",
      "‚úÖ psycopg2-binary uninstalled\n",
      "üóëÔ∏è  Uninstalling pandas...\n",
      "Found existing installation: psycopg2-binary 2.9.9\n",
      "Uninstalling psycopg2-binary-2.9.9:\n",
      "  Successfully uninstalled psycopg2-binary-2.9.9\n",
      "‚úÖ psycopg2-binary uninstalled\n",
      "üóëÔ∏è  Uninstalling pandas...\n",
      "Found existing installation: pandas 2.0.3\n",
      "Uninstalling pandas-2.0.3:\n",
      "  Successfully uninstalled pandas-2.0.3\n",
      "‚úÖ pandas uninstalled\n",
      "üóëÔ∏è  Uninstalling numpy...\n",
      "Found existing installation: pandas 2.0.3\n",
      "Uninstalling pandas-2.0.3:\n",
      "  Successfully uninstalled pandas-2.0.3\n",
      "‚úÖ pandas uninstalled\n",
      "üóëÔ∏è  Uninstalling numpy...\n",
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "‚úÖ numpy uninstalled\n",
      "\n",
      "üîß Step 2: Installing fresh packages...\n",
      "üì¶ Installing numpy==1.26.4...\n",
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "‚úÖ numpy uninstalled\n",
      "\n",
      "üîß Step 2: Installing fresh packages...\n",
      "üì¶ Installing numpy==1.26.4...\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "‚úÖ numpy==1.26.4 installed!\n",
      "üì¶ Installing pandas==2.0.3...\n",
      "Successfully installed numpy-1.26.4\n",
      "‚úÖ numpy==1.26.4 installed!\n",
      "üì¶ Installing pandas==2.0.3...\n",
      "Collecting pandas==2.0.3\n",
      "  Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Installing collected packages: pandas\n",
      "Collecting pandas==2.0.3\n",
      "  Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Using cached pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-2.0.3\n",
      "‚úÖ pandas==2.0.3 installed!\n",
      "üì¶ Installing psycopg2-binary==2.9.9...\n",
      "Successfully installed pandas-2.0.3\n",
      "‚úÖ pandas==2.0.3 installed!\n",
      "üì¶ Installing psycopg2-binary==2.9.9...\n",
      "Collecting psycopg2-binary==2.9.9\n",
      "  Using cached psycopg2_binary-2.9.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Using cached psycopg2_binary-2.9.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "Collecting psycopg2-binary==2.9.9\n",
      "  Using cached psycopg2_binary-2.9.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Using cached psycopg2_binary-2.9.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.9\n",
      "‚úÖ psycopg2-binary==2.9.9 installed!\n",
      "\n",
      "üîß Step 3: Installing dependencies...\n",
      "‚úÖ psycopg2-binary==2.9.9 installed!\n",
      "\n",
      "üîß Step 3: Installing dependencies...\n",
      "\n",
      "============================================================\n",
      "‚úÖ All dependencies installed successfully!\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: RESTART THE KERNEL NOW!\n",
      "   1. Menu ‚Üí Kernel ‚Üí Restart Kernel\n",
      "   2. After restart, skip cells 1-6\n",
      "   3. Start from Cell 7 (## 1. Environment Setup)\n",
      "   4. Execute cells 7-31 in sequence\n",
      "\n",
      "üí° TIP: If numpy 2.x gets reinstalled again, run:\n",
      "   bash silver/install_dependencies.sh\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "‚úÖ All dependencies installed successfully!\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: RESTART THE KERNEL NOW!\n",
      "   1. Menu ‚Üí Kernel ‚Üí Restart Kernel\n",
      "   2. After restart, skip cells 1-6\n",
      "   3. Start from Cell 7 (## 1. Environment Setup)\n",
      "   4. Execute cells 7-31 in sequence\n",
      "\n",
      "üí° TIP: If numpy 2.x gets reinstalled again, run:\n",
      "   bash silver/install_dependencies.sh\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for PostgreSQL connection\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def uninstall_package(package_name):\n",
    "    \"\"\"Uninstall a package completely.\"\"\"\n",
    "    try:\n",
    "        print(f\"üóëÔ∏è  Uninstalling {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package_name])\n",
    "        print(f\"‚úÖ {package_name} uninstalled\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {package_name} was not installed or error: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package with force-reinstall.\"\"\"\n",
    "    package_name = package.split('==')[0].split('<')[0].split('>')[0]\n",
    "    print(f\"üì¶ Installing {package}...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"--force-reinstall\", \"--no-deps\", package])\n",
    "        print(f\"‚úÖ {package} installed!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FIXING DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, completely remove broken packages\n",
    "print(\"\\nüîß Step 1: Removing broken installations...\")\n",
    "uninstall_package(\"psycopg2\")\n",
    "uninstall_package(\"psycopg2-binary\")\n",
    "uninstall_package(\"pandas\")\n",
    "uninstall_package(\"numpy\")\n",
    "\n",
    "# Install fresh packages in the correct order\n",
    "# CRITICAL: numpy MUST be exactly 1.26.4 (NOT 2.x!)\n",
    "print(\"\\nüîß Step 2: Installing fresh packages...\")\n",
    "packages = [\n",
    "    \"numpy==1.26.4\",  # EXACT version - 2.x is INCOMPATIBLE with pandas 2.0.3!\n",
    "    \"pandas==2.0.3\",\n",
    "    \"psycopg2-binary==2.9.9\"\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for pkg in packages:\n",
    "    if not install_package(pkg):\n",
    "        all_ok = False\n",
    "\n",
    "# Install dependencies of pandas and numpy\n",
    "if all_ok:\n",
    "    print(\"\\nüîß Step 3: Installing dependencies...\")\n",
    "    deps = [\"python-dateutil\", \"pytz\", \"tzdata\", \"six\"]\n",
    "    for dep in deps:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", dep], \n",
    "                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if all_ok:\n",
    "    print(\"‚úÖ All dependencies installed successfully!\")\n",
    "    print(\"\\n‚ö†Ô∏è  CRITICAL: RESTART THE KERNEL NOW!\")\n",
    "    print(\"   1. Menu ‚Üí Kernel ‚Üí Restart Kernel\")\n",
    "    print(\"   2. After restart, skip cells 1-6\")\n",
    "    print(\"   3. Start from Cell 7 (## 1. Environment Setup)\")\n",
    "    print(\"   4. Execute cells 7-31 in sequence\")\n",
    "    print(\"\\nüí° TIP: If numpy 2.x gets reinstalled again, run:\")\n",
    "    print(\"   bash silver/install_dependencies.sh\")\n",
    "else:\n",
    "    print(\"‚ùå Some packages failed to install\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93063d",
   "metadata": {},
   "source": [
    "# Test Postgres Insert - Silver Layer\n",
    "\n",
    "Simple test notebook to validate PostgreSQL integration with PySpark.\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Creating a test DataFrame with PySpark\n",
    "2. Connecting to PostgreSQL using postgres_helper\n",
    "3. Inserting data using cliente_postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385a055",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "**CRITICAL**: This cell ensures we use packages from the SAME Python version as the kernel.\n",
    "If the kernel uses Python 3.11, we MUST use packages from Python 3.11 site-packages.\n",
    "Using packages from a different Python version (e.g., 3.12) causes binary incompatibility errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99d3d9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Kernel using Python 3.11\n",
      "üìç Executable: /usr/bin/python3.11\n",
      "‚úÖ Prioritized user packages: /home/davi/.local/lib/python3.11/site-packages\n",
      "‚úÖ PySpark found at: /home/davi/.local/lib/python3.12/site-packages/pyspark\n",
      "‚úÖ PySpark 3.5.1 loaded successfully\n",
      "‚úÖ Java configured: /usr/lib/jvm/java-17-openjdk-amd64\n",
      "\n",
      "üìã Final sys.path (first 5):\n",
      "   0: /home/davi/.local/lib/python3.12/site-packages\n",
      "   1: /home/davi/.local/lib/python3.11/site-packages\n",
      "   2: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/plugins\n",
      "   3: /tmp/spark-2de69f26-d690-439f-bf55-45a29dde18f2/userFiles-7ede4014-2717-423a-80fd-150467d7b67a\n",
      "   4: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/helpers\n"
     ]
    }
   ],
   "source": [
    "# Configure environment for local execution\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if 'AIRFLOW_HOME' not in os.environ:\n",
    "    # Running locally - determine the CORRECT Python version being used by kernel\n",
    "    kernel_python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "    print(f\"üêç Kernel using Python {kernel_python_version}\")\n",
    "    print(f\"üìç Executable: {sys.executable}\")\n",
    "    \n",
    "    # Define user site-packages for the kernel's Python version\n",
    "    user_site_packages = Path.home() / '.local' / 'lib' / f'python{kernel_python_version}' / 'site-packages'\n",
    "    \n",
    "    # CRITICAL: Remove /usr/lib/python3/dist-packages (contains broken system numpy)\n",
    "    # and any paths from other Python versions\n",
    "    paths_to_remove = []\n",
    "    for path_str in sys.path:\n",
    "        # Remove other Python versions\n",
    "        if 'python3.' in path_str and kernel_python_version not in path_str:\n",
    "            paths_to_remove.append(path_str)\n",
    "        # Remove system dist-packages (contains broken numpy on Ubuntu 24.04)\n",
    "        elif path_str == '/usr/lib/python3/dist-packages':\n",
    "            paths_to_remove.append(path_str)\n",
    "    \n",
    "    for path in paths_to_remove:\n",
    "        try:\n",
    "            sys.path.remove(path)\n",
    "            print(f\"üóëÔ∏è  Removed incompatible path: {path}\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Ensure user site-packages is first (after current directory)\n",
    "    if user_site_packages.exists():\n",
    "        if str(user_site_packages) in sys.path:\n",
    "            sys.path.remove(str(user_site_packages))\n",
    "        sys.path.insert(0, str(user_site_packages))\n",
    "        print(f\"‚úÖ Prioritized user packages: {user_site_packages}\")\n",
    "    \n",
    "    # Try to find PySpark in multiple locations\n",
    "    pyspark_found = False\n",
    "    search_paths = [\n",
    "        user_site_packages,\n",
    "        Path.home() / '.local' / 'lib' / 'python3.12' / 'site-packages',\n",
    "        Path('/usr/local/lib') / f'python{kernel_python_version}' / 'dist-packages',\n",
    "        Path('/usr/local/lib/python3.12/dist-packages'),\n",
    "    ]\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        pyspark_path = search_path / 'pyspark'\n",
    "        if pyspark_path.exists():\n",
    "            if str(search_path) not in sys.path:\n",
    "                sys.path.insert(0, str(search_path))\n",
    "            print(f\"‚úÖ PySpark found at: {pyspark_path}\")\n",
    "            pyspark_found = True\n",
    "            break\n",
    "    \n",
    "    if not pyspark_found:\n",
    "        print(\"‚ùå PySpark not found in any location!\")\n",
    "        print(\"   Please install with: pip install --user pyspark==3.5.1\")\n",
    "    \n",
    "    # Verify import\n",
    "    try:\n",
    "        import pyspark\n",
    "        print(f\"‚úÖ PySpark {pyspark.__version__} loaded successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Error importing PySpark: {e}\")\n",
    "    \n",
    "    # Configure Java\n",
    "    java_home = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "    if os.path.exists(java_home):\n",
    "        os.environ['JAVA_HOME'] = java_home\n",
    "        print(f\"‚úÖ Java configured: {java_home}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Warning: Java 17 not found at expected location\")\n",
    "        print(\"   Install with: sudo apt install openjdk-17-jdk\")\n",
    "    \n",
    "    print(\"\\nüìã Final sys.path (first 5):\")\n",
    "    for i, p in enumerate(sys.path[:5]):\n",
    "        print(f\"   {i}: {p}\")\n",
    "else:\n",
    "    print(\"üê≥ Running in Airflow - environment already configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa268eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking numpy version...\n",
      "   Installed: numpy 1.26.4\n",
      "   Location: /home/davi/.local/lib/python3.11/site-packages/numpy/__init__.py\n",
      "‚úÖ numpy 1.26.4 is compatible with pandas 2.0.3\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL CHECK: Verify numpy version BEFORE proceeding\n",
    "import sys\n",
    "\n",
    "print(\"üîç Checking numpy version...\")\n",
    "try:\n",
    "    import numpy\n",
    "    numpy_version = numpy.__version__\n",
    "    numpy_major = int(numpy_version.split('.')[0])\n",
    "    \n",
    "    print(f\"   Installed: numpy {numpy_version}\")\n",
    "    print(f\"   Location: {numpy.__file__}\")\n",
    "    \n",
    "    if numpy_major >= 2:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚ùå CRITICAL ERROR: INCOMPATIBLE NUMPY VERSION!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Detected: numpy {numpy_version}\")\n",
    "        print(\"Required: numpy 1.26.4\")\n",
    "        print(\"\\nPandas 2.0.3 is INCOMPATIBLE with numpy 2.x!\")\n",
    "        print(\"This will cause: ValueError: numpy.dtype size changed\")\n",
    "        print(\"\\nüîß FIX:\")\n",
    "        print(\"   Option 1: Run Cell 5 (Install Dependencies), then RESTART KERNEL\")\n",
    "        print(\"   Option 2: Run in terminal:\")\n",
    "        print(\"      python3.11 -m pip uninstall -y numpy\")\n",
    "        print(\"      python3.11 -m pip install --user 'numpy==1.26.4'\")\n",
    "        print(\"   Option 3: Run script:\")\n",
    "        print(\"      bash silver/install_dependencies.sh\")\n",
    "        print(\"\\nAfter fixing, RESTART THE KERNEL before continuing!\")\n",
    "        print(\"=\"*60)\n",
    "        raise RuntimeError(f\"numpy {numpy_version} is incompatible with pandas 2.0.3\")\n",
    "    else:\n",
    "        print(f\"‚úÖ numpy {numpy_version} is compatible with pandas 2.0.3\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  numpy not installed!\")\n",
    "    print(\"   Run Cell 5 (Install Dependencies), then RESTART KERNEL\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d96b36fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2\n",
      "Spark Config Path: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/spark_config\n",
      "Postgres Helpers Path: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/helpers\n",
      "Postgres Plugins Path: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/plugins\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "if 'AIRFLOW_HOME' in os.environ:\n",
    "    # Running in Airflow\n",
    "    BASE_PATH = Path('/opt/airflow')\n",
    "    SPARK_CONFIG_PATH = BASE_PATH / 'spark_config'\n",
    "    POSTGRES_HELPERS_PATH = BASE_PATH / 'postgres' / 'helpers'\n",
    "    POSTGRES_PLUGINS_PATH = BASE_PATH / 'postgres' / 'plugins'\n",
    "else:\n",
    "    # Running manually\n",
    "    BASE_PATH = Path.cwd().parent\n",
    "    SPARK_CONFIG_PATH = BASE_PATH / 'spark_config'\n",
    "    POSTGRES_HELPERS_PATH = BASE_PATH / 'postgres' / 'helpers'\n",
    "    POSTGRES_PLUGINS_PATH = BASE_PATH / 'postgres' / 'plugins'\n",
    "\n",
    "# Add paths\n",
    "sys.path.insert(0, str(SPARK_CONFIG_PATH))\n",
    "sys.path.insert(0, str(POSTGRES_HELPERS_PATH))\n",
    "sys.path.insert(0, str(POSTGRES_PLUGINS_PATH))\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"Spark Config Path: {SPARK_CONFIG_PATH}\")\n",
    "print(f\"Postgres Helpers Path: {POSTGRES_HELPERS_PATH}\")\n",
    "print(f\"Postgres Plugins Path: {POSTGRES_PLUGINS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cee8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PostgreSQL environment variables configured\n",
      "   Database: data_warehouse\n",
      "   Host: localhost:5433\n"
     ]
    }
   ],
   "source": [
    "# Configure PostgreSQL connection for manual execution\n",
    "if 'AIRFLOW_HOME' not in os.environ:\n",
    "    # Set PostgreSQL connection environment variables\n",
    "    os.environ.setdefault('POSTGRES_DB', 'data_warehouse')\n",
    "    os.environ.setdefault('POSTGRES_USER', 'airflow')\n",
    "    os.environ.setdefault('POSTGRES_PASSWORD', 'airflow')\n",
    "    os.environ.setdefault('POSTGRES_HOST', 'localhost')\n",
    "    os.environ.setdefault('POSTGRES_PORT', '5433')\n",
    "    \n",
    "    print(\"‚úÖ PostgreSQL environment variables configured\")\n",
    "    print(f\"   Database: {os.environ['POSTGRES_DB']}\")\n",
    "    print(f\"   Host: {os.environ['POSTGRES_HOST']}:{os.environ['POSTGRES_PORT']}\")\n",
    "else:\n",
    "    print(\"üê≥ Running in Airflow - using Airflow connection settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d8caa",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a760f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config:Spark Session criada: Test_Postgres_Insert\n",
      "INFO:config:Spark UI dispon√≠vel em: http://192.168.0.12:4041\n",
      "INFO:config:Spark UI dispon√≠vel em: http://192.168.0.12:4041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n",
      "Spark UI: http://192.168.0.12:4041\n"
     ]
    }
   ],
   "source": [
    "from config import SparkConfig\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Create Spark session\n",
    "spark_config = SparkConfig(app_name=\"Test_Postgres_Insert\")\n",
    "spark = spark_config.create_spark_session()\n",
    "spark_config.configure_for_banking_data()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ca9c5",
   "metadata": {},
   "source": [
    "## 3. Create Test DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b72d677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test DataFrame created successfully!\n",
      "Total records: 5\n"
     ]
    }
   ],
   "source": [
    "# Create a simple test DataFrame\n",
    "test_data = [\n",
    "    (\"Product A\", 100, 25.50),\n",
    "    (\"Product B\", 200, 15.75),\n",
    "    (\"Product C\", 150, 30.00),\n",
    "    (\"Product D\", 300, 10.25),\n",
    "    (\"Product E\", 50, 45.99)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_test = spark.createDataFrame(test_data, schema)\n",
    "\n",
    "print(\"‚úÖ Test DataFrame created successfully!\")\n",
    "print(f\"Total records: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c739de",
   "metadata": {},
   "source": [
    "## 4. Display DataFrame Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f4a5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8cb7f",
   "metadata": {},
   "source": [
    "## 5. Show Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74314c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample records:\n",
      "+------------+--------+-----+\n",
      "|product_name|quantity|price|\n",
      "+------------+--------+-----+\n",
      "|Product A   |100     |25.5 |\n",
      "|Product B   |200     |15.75|\n",
      "|Product C   |150     |30.0 |\n",
      "|Product D   |300     |10.25|\n",
      "|Product E   |50      |45.99|\n",
      "+------------+--------+-----+\n",
      "\n",
      "+------------+--------+-----+\n",
      "|product_name|quantity|price|\n",
      "+------------+--------+-----+\n",
      "|Product A   |100     |25.5 |\n",
      "|Product B   |200     |15.75|\n",
      "|Product C   |150     |30.0 |\n",
      "|Product D   |300     |10.25|\n",
      "|Product E   |50      |45.99|\n",
      "+------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample records\n",
    "print(\"Sample records:\")\n",
    "df_test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12bf0c8",
   "metadata": {},
   "source": [
    "## 6. Connect to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "233b4776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Preparing environment for Python 3.11\n",
      "üóëÔ∏è  Removed: /home/davi/.local/lib/python3.12/site-packages\n",
      "‚úÖ Prioritized: /home/davi/.local/lib/python3.11/site-packages\n",
      "üîÑ Cleared 407 cached modules\n",
      "\n",
      "üìã sys.path (first 5):\n",
      "  0: /home/davi/.local/lib/python3.11/site-packages\n",
      "  1: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/plugins\n",
      "  2: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/helpers\n",
      "  3: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/spark_config\n",
      "  4: /home/davi/√Årea de Trabalho/bancos2/TrabalhoSBD2/postgres/plugins\n",
      "\n",
      "‚úÖ Environment ready for PostgreSQL imports\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Force remove Python 3.12 paths before importing psycopg2\n",
    "# ‚ö†Ô∏è  WARNING: ALWAYS RUN THIS CELL BEFORE IMPORTING POSTGRESQL LIBRARIES!\n",
    "# This cell MUST be executed before the next cell to prevent numpy/pandas version conflicts.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "kernel_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "print(f\"üîß Preparing environment for Python {kernel_version}\")\n",
    "\n",
    "# Define the correct user site-packages path\n",
    "user_site = str(Path.home() / '.local' / 'lib' / f'python{kernel_version}' / 'site-packages')\n",
    "\n",
    "# CRITICAL: Remove ALL problematic paths\n",
    "paths_to_remove = []\n",
    "for path_str in sys.path:\n",
    "    # Remove other Python versions (3.12, 3.13, etc.)\n",
    "    if 'python3.' in path_str and kernel_version not in path_str:\n",
    "        paths_to_remove.append(path_str)\n",
    "    # Remove /usr/lib/python3/dist-packages (BROKEN numpy on Ubuntu 24.04)\n",
    "    elif path_str == '/usr/lib/python3/dist-packages':\n",
    "        paths_to_remove.append(path_str)\n",
    "    # Remove /usr/local/lib/python3/dist-packages if it exists\n",
    "    elif path_str == '/usr/local/lib/python3/dist-packages':\n",
    "        paths_to_remove.append(path_str)\n",
    "\n",
    "for path in paths_to_remove:\n",
    "    try:\n",
    "        sys.path.remove(path)\n",
    "        print(f\"üóëÔ∏è  Removed: {path}\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Ensure Python 3.11 user site-packages is FIRST\n",
    "if user_site in sys.path:\n",
    "    sys.path.remove(user_site)\n",
    "sys.path.insert(0, user_site)\n",
    "print(f\"‚úÖ Prioritized: {user_site}\")\n",
    "\n",
    "# Clear ALL cached imports that might have incompatible binaries\n",
    "modules_to_clear = [\n",
    "    'psycopg2', 'psycopg2._psycopg', 'psycopg2.extras',\n",
    "    'pandas', 'pandas.core', 'pandas._libs',\n",
    "    'numpy', 'numpy.core', 'numpy.core._multiarray_umath',\n",
    "    'cliente_postgres', 'postgres_helper'\n",
    "]\n",
    "\n",
    "cleared_count = 0\n",
    "for module_name in list(sys.modules.keys()):\n",
    "    # Clear exact matches or submodules\n",
    "    if module_name in modules_to_clear or any(module_name.startswith(m + '.') for m in modules_to_clear):\n",
    "        del sys.modules[module_name]\n",
    "        cleared_count += 1\n",
    "\n",
    "print(f\"üîÑ Cleared {cleared_count} cached modules\")\n",
    "\n",
    "print(\"\\nüìã sys.path (first 5):\")\n",
    "for i, p in enumerate(sys.path[:5]):\n",
    "    print(f\"  {i}: {p}\")\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for PostgreSQL imports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c89e53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davi/.local/lib/python3.11/site-packages/pandas/__init__.py:11: UserWarning: The NumPy module was reloaded (imported a second time). This can in some cases result in small but subtle issues and is discouraged.\n",
      "  __import__(_dependency)\n",
      "INFO:root:[postgres_helpers] Using manual PostgreSQL connection: dbname=data_warehouse, user=airflow, host=localhost, port=5433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PostgreSQL connection string obtained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[cliente_postgres.py] Initialized ClientPostgresDB with conn_str: dbname=data_warehouse user=airflow password=airflow host=localhost port=5433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PostgreSQL client created\n"
     ]
    }
   ],
   "source": [
    "# Import postgres helper and client\n",
    "from postgres_helper import get_postgres_conn\n",
    "from cliente_postgres import ClientPostgresDB\n",
    "\n",
    "# Get connection string\n",
    "conn_str = get_postgres_conn()\n",
    "print(\"‚úÖ PostgreSQL connection string obtained\")\n",
    "\n",
    "# Create client\n",
    "postgres_client = ClientPostgresDB(conn_str)\n",
    "print(\"‚úÖ PostgreSQL client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df5d1b",
   "metadata": {},
   "source": [
    "## 7. Convert DataFrame to Dictionary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4edae5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data converted successfully!\n",
      "Number of records: 5\n",
      "Sample record: {'product_name': 'Product A', 'quantity': 100, 'price': 25.5}\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark DataFrame to list of dictionaries\n",
    "rows = df_test.collect()\n",
    "data_to_insert = [row.asDict() for row in rows]\n",
    "\n",
    "print(f\"‚úÖ Data converted successfully!\")\n",
    "print(f\"Number of records: {len(data_to_insert)}\")\n",
    "print(f\"Sample record: {data_to_insert[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118ec66",
   "metadata": {},
   "source": [
    "## 8. Insert Data into PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0dfd553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[cliente_postgres.py] Schema silver ensured to exist\n",
      "INFO:root:[cliente_postgres.py] Table silver.test_products created or already exists\n",
      "INFO:root:[cliente_postgres.py] Table silver.test_products created or already exists\n",
      "INFO:root:[cliente_postgres.py] Inserted data into silver.test_products\n",
      "INFO:root:[cliente_postgres.py] Inserted data into silver.test_products\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data inserted into silver.test_products\n"
     ]
    }
   ],
   "source": [
    "# Define table name and schema\n",
    "table_name = \"test_products\"\n",
    "schema_name = \"silver\"\n",
    "\n",
    "# Insert data\n",
    "postgres_client.insert_data(\n",
    "    data=data_to_insert,\n",
    "    table_name=table_name,\n",
    "    schema=schema_name,\n",
    "    primary_key=[\"product_name\"],\n",
    "    conflict_fields=[\"product_name\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data inserted into {schema_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f0321d",
   "metadata": {},
   "source": [
    "## 9. Verify Data Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e302d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:[cliente_postgres.py] Executing query: SELECT * FROM silver.test_products ORDER BY product_name;\n",
      "INFO:root:[cliente_postgres.py] Query executed successfully, fetched 5 rows\n",
      "INFO:root:[cliente_postgres.py] Query executed successfully, fetched 5 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data retrieved from silver.test_products\n",
      "Total records in database: 5\n",
      "\n",
      "Data in database:\n",
      "('Product A', '100', '25.5')\n",
      "('Product B', '200', '15.75')\n",
      "('Product C', '150', '30.0')\n",
      "('Product D', '300', '10.25')\n",
      "('Product E', '50', '45.99')\n"
     ]
    }
   ],
   "source": [
    "# Query the data\n",
    "query = f\"SELECT * FROM {schema_name}.{table_name} ORDER BY product_name;\"\n",
    "results = postgres_client.execute_query(query)\n",
    "\n",
    "print(f\"‚úÖ Data retrieved from {schema_name}.{table_name}\")\n",
    "print(f\"Total records in database: {len(results)}\")\n",
    "print(\"\\nData in database:\")\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f22bb",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec163ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST SUMMARY\n",
      "============================================================\n",
      "DataFrame records created: 5\n",
      "Records inserted to database: 5\n",
      "Records retrieved from database: 5\n",
      "Table: silver.test_products\n",
      "============================================================\n",
      "‚úÖ All operations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"DataFrame records created: {len(test_data)}\")\n",
    "print(f\"Records inserted to database: {len(data_to_insert)}\")\n",
    "print(f\"Records retrieved from database: {len(results)}\")\n",
    "print(f\"Table: {schema_name}.{table_name}\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ All operations completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5577ec5",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c02908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config:Spark Session finalizada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark_config.stop_session()\n",
    "print(\"Test completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
