{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad382fe",
   "metadata": {},
   "source": [
    "# BOP Data Analysis - Silver Layer\n",
    "\n",
    "Analysis of Balance of Payments (BOP) data using PySpark.\n",
    "\n",
    "This notebook performs basic exploratory analysis on BOP data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e51eed",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82634f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PySpark found at: /home/davi/.local/lib/python3.12/site-packages\n",
      "âœ… PySpark 3.5.1 loaded successfully\n",
      "âœ… Java configured: /usr/lib/jvm/java-17-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "# Configure environment for local execution\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'AIRFLOW_HOME' not in os.environ:\n",
    "    # Running locally - add PySpark from user installation to path\n",
    "    pyspark_paths = [\n",
    "        '/home/davi/.local/lib/python3.12/site-packages',  # Python 3.12 user site\n",
    "        '/home/davi/.local/lib/python3.11/site-packages',  # Python 3.11 user site\n",
    "        '/usr/local/lib/python3.12/dist-packages',         # System Python 3.12\n",
    "        '/usr/local/lib/python3.11/dist-packages',         # System Python 3.11\n",
    "    ]\n",
    "    \n",
    "    pyspark_found = False\n",
    "    for path in pyspark_paths:\n",
    "        if os.path.exists(os.path.join(path, 'pyspark')):\n",
    "            if path not in sys.path:\n",
    "                sys.path.insert(0, path)\n",
    "            pyspark_found = True\n",
    "            print(f\"âœ… PySpark found at: {path}\")\n",
    "            break\n",
    "    \n",
    "    if not pyspark_found:\n",
    "        print(\"âš ï¸  PySpark not found. Installing...\")\n",
    "        import subprocess\n",
    "        # Install for the current Python version\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", \"pyspark==3.5.1\", \"pyarrow==15.0.2\", \"findspark==2.0.1\"])\n",
    "        print(\"âœ… PySpark installed successfully!\")\n",
    "    \n",
    "    # Verify import\n",
    "    try:\n",
    "        import pyspark\n",
    "        print(f\"âœ… PySpark {pyspark.__version__} loaded successfully\")\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Error importing PySpark: {e}\")\n",
    "    \n",
    "    # Configure Java\n",
    "    java_home = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "    if os.path.exists(java_home):\n",
    "        os.environ['JAVA_HOME'] = java_home\n",
    "        print(f\"âœ… Java configured: {java_home}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Warning: Java 17 not found at expected location\")\n",
    "        print(\"   Install with: sudo apt install openjdk-17-jdk\")\n",
    "else:\n",
    "    print(\"ðŸ³ Running in Airflow - environment already configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cf2263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /home/mateus/Documentos/Faculdade/bancos2/TrabalhoSBD2\n",
      "Data Path: /home/mateus/Documentos/Faculdade/bancos2/TrabalhoSBD2/base_dados/Resultados\n",
      "Spark Config Path: /home/mateus/Documentos/Faculdade/bancos2/TrabalhoSBD2/spark_config\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "if 'AIRFLOW_HOME' in os.environ:\n",
    "    # Running in Airflow\n",
    "    BASE_PATH = Path('/opt/airflow')\n",
    "    DATA_PATH = BASE_PATH / 'base_dados' / 'Resultados'\n",
    "    SPARK_CONFIG_PATH = BASE_PATH / 'spark_config'\n",
    "else:\n",
    "    # Running manually\n",
    "    BASE_PATH = Path.cwd().parent\n",
    "    DATA_PATH = BASE_PATH / 'base_dados' / 'Resultados'\n",
    "    SPARK_CONFIG_PATH = BASE_PATH / 'spark_config'\n",
    "\n",
    "# Add spark_config to path\n",
    "sys.path.insert(0, str(SPARK_CONFIG_PATH))\n",
    "\n",
    "print(f\"Base Path: {BASE_PATH}\")\n",
    "print(f\"Data Path: {DATA_PATH}\")\n",
    "print(f\"Spark Config Path: {SPARK_CONFIG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60e331",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4202999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 12:04:39 WARN Utils: Your hostname, mateus-730QED resolves to a loopback address: 127.0.1.1; using 172.29.99.42 instead (on interface wlo1)\n",
      "25/10/17 12:04:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/17 12:04:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "INFO:config:Spark Session criada: BOP_Silver_Analysis\n",
      "INFO:config:Spark UI disponÃ­vel em: http://172.29.99.42:4040\n",
      "25/10/17 12:04:44 WARN SQLConf: The SQL config 'spark.sql.adaptive.coalescePartitions.minPartitionNum' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead.\n",
      "25/10/17 12:04:44 WARN SQLConf: The SQL config 'spark.sql.adaptive.coalescePartitions.minPartitionNum' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead.\n",
      "25/10/17 12:04:44 WARN SQLConf: The SQL config 'spark.sql.adaptive.coalescePartitions.minPartitionNum' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n",
      "Spark UI: http://172.29.99.42:4040\n"
     ]
    }
   ],
   "source": [
    "from config import SparkConfig, DataSchemas\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create Spark session\n",
    "spark_config = SparkConfig(app_name=\"BOP_Silver_Analysis\")\n",
    "spark = spark_config.create_spark_session()\n",
    "spark_config.configure_for_banking_data()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c3e6b",
   "metadata": {},
   "source": [
    "## 3. Load BOP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758712f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: /home/davi/Ãrea de Trabalho/bancos2/TrabalhoSBD2/base_dados/Resultados/BOP.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/09 15:16:55 WARN SQLConf: The SQL config 'spark.sql.adaptive.coalescePartitions.minPartitionNum' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead.\n",
      "25/10/09 15:16:55 WARN SQLConf: The SQL config 'spark.sql.adaptive.coalescePartitions.minPartitionNum' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead.\n",
      "25/10/09 15:16:55 WARN SQLConf: The SQL config 'spark.sql.adaptive.coalescePartitions.minPartitionNum' has been deprecated in Spark v3.2 and may be removed in the future. Use 'spark.sql.adaptive.coalescePartitions.minPartitionSize' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 180,804\n"
     ]
    }
   ],
   "source": [
    "# Load BOP data\n",
    "bop_file = DATA_PATH / 'BOP.csv'\n",
    "print(f\"Loading file: {bop_file}\")\n",
    "\n",
    "df_bop = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(str(bop_file))\n",
    "\n",
    "print(f\"Total records loaded: {df_bop.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec998cda",
   "metadata": {},
   "source": [
    "## 4. Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb60eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- BOP_ACCOUNTING_ENTRY: string (nullable = true)\n",
      " |-- INDICATOR: string (nullable = true)\n",
      " |-- UNIT: string (nullable = true)\n",
      " |-- FREQUENCY: string (nullable = true)\n",
      " |-- TIME_PERIOD: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      " |-- index: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema\n",
    "df_bop.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a719f",
   "metadata": {},
   "source": [
    "## 5. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ef80a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/09 15:17:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 5:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+---------+------+---------+-----------+--------------------+-----+\n",
      "|summary|COUNTRY|BOP_ACCOUNTING_ENTRY|INDICATOR|  UNIT|FREQUENCY|TIME_PERIOD|               value|index|\n",
      "+-------+-------+--------------------+---------+------+---------+-----------+--------------------+-----+\n",
      "|  count| 180804|              180804|   180804|180804|   180804|     180804|              180804|    0|\n",
      "|   mean|   NULL|                NULL|     NULL|  NULL|     NULL|       NULL| 8.002790936956046E9| NULL|\n",
      "| stddev|   NULL|                NULL|     NULL|  NULL|     NULL|       NULL|4.131887512070207E10| NULL|\n",
      "|    min|    ARG|             A_NFA_T|      CAB|   USD|        Q|    2000-Q1|-6.73055075813792...| NULL|\n",
      "|    max|    ZAF|          NNAFANIL_T|       SF|   USD|        Q|    2025-Q2|         1.158498E12| NULL|\n",
      "+-------+-------+--------------------+---------+------+---------+-----------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "df_bop.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d2e0a1",
   "metadata": {},
   "source": [
    "## 6. Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5d6796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample records:\n",
      "+-------+--------------------+---------+----+---------+-----------+-----------+-----+\n",
      "|COUNTRY|BOP_ACCOUNTING_ENTRY|INDICATOR|UNIT|FREQUENCY|TIME_PERIOD|value      |index|\n",
      "+-------+--------------------+---------+----+---------+-----------+-----------+-----+\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2000-Q1    |-8.4585E10 |NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2000-Q2    |-9.6104E10 |NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2000-Q3    |-1.14965E11|NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2000-Q4    |-1.06274E11|NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2001-Q1    |-9.6454E10 |NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2001-Q2    |-9.5128E10 |NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2001-Q3    |-1.13956E11|NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2001-Q4    |-8.8541E10 |NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2002-Q1    |-9.0445E10 |NULL |\n",
      "|USA    |NETCD_T             |CAB      |USD |Q        |2002-Q2    |-1.14317E11|NULL |\n",
      "+-------+--------------------+---------+----+---------+-----------+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample records\n",
    "print(\"Sample records:\")\n",
    "df_bop.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fef58",
   "metadata": {},
   "source": [
    "## 7. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e302882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records by country:\n",
      "+-------+-----+\n",
      "|COUNTRY|count|\n",
      "+-------+-----+\n",
      "|    KOR| 3978|\n",
      "|    BRA| 3978|\n",
      "|    MEX| 3978|\n",
      "|    ITA| 3939|\n",
      "|    GBR| 3939|\n",
      "|    FIN| 3939|\n",
      "|    NLD| 3939|\n",
      "|    POL| 3939|\n",
      "|    CZE| 3939|\n",
      "|    AUS| 3939|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+\n",
      "|COUNTRY|count|\n",
      "+-------+-----+\n",
      "|    KOR| 3978|\n",
      "|    BRA| 3978|\n",
      "|    MEX| 3978|\n",
      "|    ITA| 3939|\n",
      "|    GBR| 3939|\n",
      "|    FIN| 3939|\n",
      "|    NLD| 3939|\n",
      "|    POL| 3939|\n",
      "|    CZE| 3939|\n",
      "|    AUS| 3939|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count records by country\n",
    "print(\"Records by country:\")\n",
    "df_bop.groupBy(\"COUNTRY\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b8bece8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records by indicator:\n",
      "+---------+-----+\n",
      "|INDICATOR|count|\n",
      "+---------+-----+\n",
      "|      IN1|14430|\n",
      "|       GS|14430|\n",
      "|      IN2|14409|\n",
      "|     O_F2|13852|\n",
      "|     O_F4|13833|\n",
      "|       SF|13748|\n",
      "|    O_F81|12821|\n",
      "|     P_F5| 9084|\n",
      "|     P_F3| 8928|\n",
      "|     D_F5| 8923|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+-----+\n",
      "|INDICATOR|count|\n",
      "+---------+-----+\n",
      "|      IN1|14430|\n",
      "|       GS|14430|\n",
      "|      IN2|14409|\n",
      "|     O_F2|13852|\n",
      "|     O_F4|13833|\n",
      "|       SF|13748|\n",
      "|    O_F81|12821|\n",
      "|     P_F5| 9084|\n",
      "|     P_F3| 8928|\n",
      "|     D_F5| 8923|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count records by indicator\n",
    "print(\"Records by indicator:\")\n",
    "df_bop.groupBy(\"INDICATOR\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebff4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records by frequency:\n",
      "+---------+------+\n",
      "|FREQUENCY| count|\n",
      "+---------+------+\n",
      "|        Q|180804|\n",
      "+---------+------+\n",
      "\n",
      "+---------+------+\n",
      "|FREQUENCY| count|\n",
      "+---------+------+\n",
      "|        Q|180804|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count records by frequency\n",
    "print(\"Records by frequency:\")\n",
    "df_bop.groupBy(\"FREQUENCY\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb8549",
   "metadata": {},
   "source": [
    "## 8. Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "569c8a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts:\n",
      "+-------+--------------------+---------+----+---------+-----------+-----+------+\n",
      "|COUNTRY|BOP_ACCOUNTING_ENTRY|INDICATOR|UNIT|FREQUENCY|TIME_PERIOD|value| index|\n",
      "+-------+--------------------+---------+----+---------+-----------+-----+------+\n",
      "|      0|                   0|        0|   0|        0|          0|    0|180804|\n",
      "+-------+--------------------+---------+----+---------+-----------+-----+------+\n",
      "\n",
      "+-------+--------------------+---------+----+---------+-----------+-----+------+\n",
      "|COUNTRY|BOP_ACCOUNTING_ENTRY|INDICATOR|UNIT|FREQUENCY|TIME_PERIOD|value| index|\n",
      "+-------+--------------------+---------+----+---------+-----------+-----+------+\n",
      "|      0|                   0|        0|   0|        0|          0|    0|180804|\n",
      "+-------+--------------------+---------+----+---------+-----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Null value counts:\")\n",
    "df_bop.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_bop.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a0a71bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value column statistics:\n",
      "+-------+--------------------+\n",
      "|summary|               value|\n",
      "+-------+--------------------+\n",
      "|  count|              180804|\n",
      "|   mean| 8.002790936956046E9|\n",
      "| stddev|4.131887512070207E10|\n",
      "|    min|-6.73055075813792...|\n",
      "|    max|         1.158498E12|\n",
      "+-------+--------------------+\n",
      "\n",
      "+-------+--------------------+\n",
      "|summary|               value|\n",
      "+-------+--------------------+\n",
      "|  count|              180804|\n",
      "|   mean| 8.002790936956046E9|\n",
      "| stddev|4.131887512070207E10|\n",
      "|    min|-6.73055075813792...|\n",
      "|    max|         1.158498E12|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistics on value column\n",
    "print(\"Value column statistics:\")\n",
    "df_bop.select(\"value\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdc4ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value statistics by unit:\n",
      "+----+------+---------------+-------------------+-----------+\n",
      "|UNIT| count|      avg_value|          min_value|  max_value|\n",
      "+----+------+---------------+-------------------+-----------+\n",
      "| USD|180804|8.00279093696E9|-6.7305507581379E11|1.158498E12|\n",
      "+----+------+---------------+-------------------+-----------+\n",
      "\n",
      "+----+------+---------------+-------------------+-----------+\n",
      "|UNIT| count|      avg_value|          min_value|  max_value|\n",
      "+----+------+---------------+-------------------+-----------+\n",
      "| USD|180804|8.00279093696E9|-6.7305507581379E11|1.158498E12|\n",
      "+----+------+---------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Value distribution by unit\n",
    "print(\"Value statistics by unit:\")\n",
    "df_bop.groupBy(\"UNIT\") \\\n",
    "    .agg(\n",
    "        F.count(\"value\").alias(\"count\"),\n",
    "        F.round(F.avg(\"value\"), 2).alias(\"avg_value\"),\n",
    "        F.round(F.min(\"value\"), 2).alias(\"min_value\"),\n",
    "        F.round(F.max(\"value\"), 2).alias(\"max_value\")\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4657a4",
   "metadata": {},
   "source": [
    "## 9. Time Period Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "553edeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records by year:\n",
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2000| 5888|\n",
      "|2001| 5916|\n",
      "|2002| 6232|\n",
      "|2003| 6260|\n",
      "|2004| 6292|\n",
      "|2005| 6804|\n",
      "|2006| 6870|\n",
      "|2007| 6873|\n",
      "|2008| 6934|\n",
      "|2009| 6948|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|2000| 5888|\n",
      "|2001| 5916|\n",
      "|2002| 6232|\n",
      "|2003| 6260|\n",
      "|2004| 6292|\n",
      "|2005| 6804|\n",
      "|2006| 6870|\n",
      "|2007| 6873|\n",
      "|2008| 6934|\n",
      "|2009| 6948|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract year from TIME_PERIOD\n",
    "df_bop_with_year = df_bop.withColumn(\n",
    "    \"year\", \n",
    "    F.substring(F.col(\"TIME_PERIOD\"), 1, 4).cast(\"int\")\n",
    ")\n",
    "\n",
    "# Records by year\n",
    "print(\"Records by year:\")\n",
    "df_bop_with_year.groupBy(\"year\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"year\") \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5061c",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80ec0714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BOP DATA ANALYSIS SUMMARY\n",
      "============================================================\n",
      "Total Records: 180,804\n",
      "Total Countries: 51\n",
      "Total Indicators: 21\n",
      "Date Range: 2000-Q1 to 2025-Q2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary report\n",
    "total_records = df_bop.count()\n",
    "total_countries = df_bop.select(\"COUNTRY\").distinct().count()\n",
    "total_indicators = df_bop.select(\"INDICATOR\").distinct().count()\n",
    "date_range = df_bop.select(\n",
    "    F.min(\"TIME_PERIOD\").alias(\"min_date\"),\n",
    "    F.max(\"TIME_PERIOD\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BOP DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Records: {total_records:,}\")\n",
    "print(f\"Total Countries: {total_countries}\")\n",
    "print(f\"Total Indicators: {total_indicators}\")\n",
    "print(f\"Date Range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e92597",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a22400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:config:Spark Session finalizada\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark_config.stop_session()\n",
    "print(\"Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644f0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
