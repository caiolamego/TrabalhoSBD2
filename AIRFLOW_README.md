# Trabalho SBD2 - Coleta Automatizada de Dados FMI

Este projeto automatiza a coleta diÃ¡ria de dados econÃ´micos do FMI (Fundo MonetÃ¡rio Internacional) usando Apache Airflow e Jupyter Notebooks.

## ğŸ“Š Bases de Dados Coletadas

O projeto coleta automaticamente dados de 4 bases do FMI:

1. **BOP** - Balance of Payments (BalanÃ§a de Pagamentos)
2. **ER** - Exchange Rate (Taxa de CÃ¢mbio)
3. **IIP** - International Investment Position (PosiÃ§Ã£o Internacional de Investimento)
4. **IRFCL** - International Reserves and Foreign Currency Liquidity (Reservas Internacionais e Liquidez em Moeda Estrangeira)

## ğŸš€ Como Usar

### PrÃ©-requisitos

- Docker
- Docker Compose
- Git (opcional)

### Iniciar o Ambiente

1. Clone o repositÃ³rio (se ainda nÃ£o tiver):
```bash
git clone <seu-repositorio>
cd TrabalhoSBD2
```

2. DÃª permissÃ£o de execuÃ§Ã£o ao script de inicializaÃ§Ã£o:
```bash
chmod +x init-airflow.sh
```

3. Execute o script de inicializaÃ§Ã£o:
```bash
./init-airflow.sh
```

Este script irÃ¡:
- Limpar qualquer instalaÃ§Ã£o anterior
- Inicializar o banco de dados PostgreSQL
- Criar o usuÃ¡rio admin do Airflow
- Subir todos os serviÃ§os (Airflow Webserver, Scheduler e PostgreSQL)
- Instalar automaticamente as dependÃªncias Python necessÃ¡rias (sdmx1, pandas, papermill, ipykernel, jupyter)

### Acessar a Interface Web

ApÃ³s a inicializaÃ§Ã£o (aguarde cerca de 1-2 minutos), acesse:

**URL**: http://localhost:8080

**Credenciais**:
- UsuÃ¡rio: `airflow`
- Senha: `airflow`

## ğŸ“… Agendamento das DAGs

Todas as DAGs estÃ£o configuradas para executar diariamente:

| DAG | HorÃ¡rio | DescriÃ§Ã£o |
|-----|---------|-----------|
| `bop_data_collection` | 02:00 | Coleta dados BOP |
| `er_data_collection` | 02:15 | Coleta dados de Taxa de CÃ¢mbio |
| `iip_data_collection` | 02:30 | Coleta dados IIP |
| `irfcl_data_collection` | 02:45 | Coleta dados IRFCL |

## ğŸ“‚ Estrutura do Projeto

```
TrabalhoSBD2/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/              # DAGs do Airflow
â”‚   â”‚   â”œâ”€â”€ bop_dag.py
â”‚   â”‚   â”œâ”€â”€ er_dag.py
â”‚   â”‚   â”œâ”€â”€ iip_dag.py
â”‚   â”‚   â””â”€â”€ irfcl_dag.py
â”‚   â”œâ”€â”€ logs/              # Logs de execuÃ§Ã£o
â”‚   â”œâ”€â”€ plugins/           # Plugins do Airflow
â”‚   â””â”€â”€ config/            # ConfiguraÃ§Ãµes
â”œâ”€â”€ base_dados/
â”‚   â”œâ”€â”€ BOP/
â”‚   â”‚   â””â”€â”€ 2_coleta.ipynb
â”‚   â”œâ”€â”€ ER/
â”‚   â”‚   â””â”€â”€ 2_coleta.ipynb
â”‚   â”œâ”€â”€ IIP/
â”‚   â”‚   â””â”€â”€ 2_coleta.ipynb
â”‚   â””â”€â”€ IRFCL/
â”‚       â””â”€â”€ 2_coleta.ipynb
â”œâ”€â”€ Resultados/            # Arquivos CSV gerados
â”‚   â”œâ”€â”€ BOP.csv
â”‚   â”œâ”€â”€ ER.csv
â”‚   â”œâ”€â”€ IIP.csv
â”‚   â””â”€â”€ IRFCL.csv
â”œâ”€â”€ docker-compose.yml     # ConfiguraÃ§Ã£o do Docker
â”œâ”€â”€ .env                   # VariÃ¡veis de ambiente
â””â”€â”€ start-airflow.sh       # Script de inicializaÃ§Ã£o
```

## ğŸ”§ Comandos Ãšteis

### Gerenciar os ServiÃ§os

```bash
# Ver status dos containers
docker-compose -f docker-compose-simple.yml ps

# Ver logs em tempo real
docker-compose -f docker-compose-simple.yml logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker-compose -f docker-compose-simple.yml logs -f airflow-scheduler

# Parar todos os serviÃ§os
docker-compose -f docker-compose-simple.yml down

# Reiniciar os serviÃ§os
docker-compose -f docker-compose-simple.yml restart

# Limpar tudo (incluindo volumes)
docker-compose -f docker-compose-simple.yml down -v
```

### Executar uma DAG Manualmente

1. Acesse a interface web (http://localhost:8080)
2. Encontre a DAG desejada na lista
3. Clique no botÃ£o â–¶ï¸ (Play) Ã  direita da DAG
4. Selecione "Trigger DAG"

Ou via linha de comando:

```bash
# Executar a DAG BOP
docker-compose -f docker-compose-simple.yml exec airflow-scheduler airflow dags trigger bop_data_collection

# Executar a DAG ER
docker-compose -f docker-compose-simple.yml exec airflow-scheduler airflow dags trigger er_data_collection

# Executar a DAG IIP
docker-compose -f docker-compose-simple.yml exec airflow-scheduler airflow dags trigger iip_data_collection

# Executar a DAG IRFCL
docker-compose -f docker-compose-simple.yml exec airflow-scheduler airflow dags trigger irfcl_data_collection
```

## ğŸ› Troubleshooting

### Os serviÃ§os nÃ£o sobem

1. Verifique se as portas 8080 e 5432 nÃ£o estÃ£o em uso:
```bash
sudo lsof -i :8080
sudo lsof -i :5432
```

2. Limpe os containers antigos:
```bash
docker-compose -f docker-compose-simple.yml down -v
./init-airflow.sh
```

### DAG nÃ£o aparece na interface

1. Verifique os logs do scheduler:
```bash
docker-compose -f docker-compose-simple.yml logs airflow-scheduler
```

2. Verifique se hÃ¡ erros de sintaxe nas DAGs:
```bash
docker-compose -f docker-compose-simple.yml exec airflow-scheduler airflow dags list
```

### Notebook nÃ£o executa

1. Verifique se as dependÃªncias estÃ£o instaladas (sdmx1, pandas, papermill)
2. Verifique os logs da execuÃ§Ã£o na pasta `airflow/logs/`
3. Execute manualmente para debug:
```bash
docker-compose -f docker-compose-simple.yml exec airflow-scheduler python -c "import sdmx, pandas, papermill; print('OK')"
```

## ğŸ“¦ DependÃªncias

As seguintes bibliotecas Python sÃ£o instaladas automaticamente no container do Airflow:

- `sdmx1` - Para coletar dados do FMI
- `pandas` - Para manipulaÃ§Ã£o de dados
- `papermill` - Para executar notebooks programaticamente
- `ipykernel` - Kernel Python para notebooks
- `jupyter` - Ambiente Jupyter

## ğŸ” SeguranÃ§a

**IMPORTANTE**: As credenciais padrÃ£o (`airflow/airflow`) sÃ£o apenas para desenvolvimento local. Para produÃ§Ã£o:

1. Altere as credenciais no arquivo `.env`:
```bash
_AIRFLOW_WWW_USER_USERNAME=seu_usuario
_AIRFLOW_WWW_USER_PASSWORD=sua_senha_forte
```

2. Gere uma nova Fernet Key:
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

3. Adicione a key no `docker-compose.yml` em `AIRFLOW__CORE__FERNET_KEY`

## ğŸ“ Notas

- Os notebooks originais em `base_dados/*/2_coleta.ipynb` nÃ£o sÃ£o modificados
- Cada execuÃ§Ã£o gera um novo notebook de output com timestamp em `airflow/logs/`
- Os arquivos CSV sÃ£o sobrescritos a cada execuÃ§Ã£o em `Resultados/`
- O Airflow usa PostgreSQL como backend database
- O executor configurado Ã© `LocalExecutor` (adequado para ambiente local/pequeno)

## ğŸ¤ Contribuindo

Para adicionar novas DAGs ou modificar as existentes, edite os arquivos em `airflow/dags/`.

## ğŸ“„ LicenÃ§a

Este projeto Ã© parte do Trabalho SBD2.
