

# Projeto ETL - Dados do FMI (TrabalhoSBD2)

[![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.9.1-blue)](https://airflow.apache.org/)
[![Spark](https://img.shields.io/badge/Apache%20Spark-3.5.1-orange)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.12-green)](https://python.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue)](https://docker.com/)

> **Objetivo:** Desenvolver um **pipeline de ETL (Extract, Transform, Load)** automatizado para **coleta, processamento e an√°lise de dados do Fundo Monet√°rio Internacional (FMI)**, utilizando **Apache Airflow** e **Apache Spark**.

---

## Vis√£o Geral

Este projeto implementa uma arquitetura moderna de **ETL orientada a dados**, capaz de coletar informa√ß√µes diretamente da **API SDMX do FMI**, transform√°-las via **Spark** e armazenar os resultados em formato anal√≠tico (Parquet, CSV e JSON).
Todo o fluxo √© **automatizado pelo Airflow**, garantindo escalabilidade, reprodutibilidade e rastreabilidade completa do processo.

---

## Arquitetura do Sistema

```
TrabalhoSBD2/
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestra√ß√£o dos servi√ßos (Airflow, Spark, Postgres)
‚îú‚îÄ‚îÄ Makefile               # Automa√ß√£o de build e execu√ß√£o
‚îú‚îÄ‚îÄ .env                    # Vari√°veis de ambiente
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îÇ
‚îú‚îÄ‚îÄ airflow/                # Configura√ß√µes e DAGs do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ dags/                  # Pipelines ETL (coleta, limpeza e transforma√ß√£o)
‚îÇ   ‚îú‚îÄ‚îÄ logs/                  # Logs de execu√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ plugins/               # Plugins e hooks customizados
‚îÇ   ‚îî‚îÄ‚îÄ config/                # Configura√ß√µes adicionais
‚îÇ
‚îú‚îÄ‚îÄ base_dados/             # Dados brutos e processados
‚îÇ   ‚îú‚îÄ‚îÄ BOP/                   # Balance of Payments
‚îÇ   ‚îú‚îÄ‚îÄ ER/                    # Exchange Rates
‚îÇ   ‚îú‚îÄ‚îÄ IIP/                   # International Investment Position
‚îÇ   ‚îú‚îÄ‚îÄ IRFCL/                 # International Reserves
‚îÇ   ‚îî‚îÄ‚îÄ Resultados/            # Sa√≠das finais dos pipelines
‚îÇ
‚îú‚îÄ‚îÄ spark_config/           # Configura√ß√µes e scripts Spark
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks anal√≠ticos
‚îî‚îÄ‚îÄ Resultados/             # Indicadores agregados e relat√≥rios
```

---

## Tecnologias Principais

| Camada             | Tecnologia                                    | Descri√ß√£o                              |
| ------------------ | --------------------------------------------- | -------------------------------------- |
| **Orquestra√ß√£o**   | [Apache Airflow](https://airflow.apache.org/) | Agendamento e automa√ß√£o dos pipelines  |
| **Processamento**  | [Apache Spark](https://spark.apache.org/)     | Transforma√ß√µes e an√°lises distribu√≠das |
| **Armazenamento**  | PostgreSQL / Parquet / CSV / JSON             | Dados intermedi√°rios e resultados      |
| **Infraestrutura** | Docker Compose                                | Ambientes isolados e reproduz√≠veis     |
| **Linguagem**      | Python 3.12                                   | Scripts ETL, an√°lise e automa√ß√£o       |

---

## Como Executar

### Pr√©-requisitos

* Docker e Docker Compose instalados
* 8GB+ RAM dispon√≠vel
* Portas **8081** (Airflow) e **5433** (PostgreSQL) livres

### Instala√ß√£o

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/seu-usuario/TrabalhoSBD2.git
cd TrabalhoSBD2

# 2. Inicialize o ambiente completo
make init
```

Ap√≥s o processo, os servi√ßos estar√£o dispon√≠veis em:

* **Airflow Web UI:** [http://localhost:8081](http://localhost:8081)
 Usu√°rio: `admin` | üîê Senha: `admin123`
* **PostgreSQL:** `localhost:5433`
* **Data Warehouse:** `data_warehouse` (schemas: staging, bronze, silver, gold)

---
 **Conex√£o PostgreSQL**                  

 **Host:**          localhost            
 **Port:**          5433                 
 **Database:**      data_warehouse       
 **Username:**      airflow              
 **Password:**      airflow              
 **Show all databases:** (marcado)       

---

## Pipelines ETL

Os pipelines s√£o executados automaticamente pelo Airflow em hor√°rios configurados.
Cada DAG representa uma coleta e transforma√ß√£o de um dataset espec√≠fico do FMI:

| DAG                     | Fonte FMI | Descri√ß√£o                             | Hor√°rio |
| ----------------------- | --------- | ------------------------------------- | ------- |
| `bop_data_collection`   | BOP       | Balan√ßa de Pagamentos                 | 02:00   |
| `er_data_collection`    | ER        | Taxas de C√¢mbio                       | 02:15   |
| `iip_data_collection`   | IIP       | Posi√ß√£o de Investimento Internacional | 02:30   |
| `irfcl_data_collection` | IRFCL     | Reservas Internacionais               | 02:45   |

---

## Processamento de Dados (Spark)

* Execu√ß√£o distribu√≠da em cluster Spark local
* Transforma√ß√µes estat√≠sticas e temporais
* Detec√ß√£o autom√°tica de outliers
* Enriquecimento de dados e agrega√ß√µes
* Exporta√ß√£o para m√∫ltiplos formatos (Parquet, CSV, JSON)

Exemplo simplificado:

```python
from spark_config.config import get_banking_spark_session
import pyspark.sql.functions as F

spark = get_banking_spark_session("Analise_FMI")

df = spark.read.csv("/opt/airflow/base_dados/Resultados/BOP.csv", header=True, inferSchema=True)

result = (df.groupBy("COUNTRY")
          .agg(F.sum("VALUE").alias("total_valor"),
               F.avg("VALUE").alias("media_valor"))
          .orderBy(F.desc("total_valor")))

result.show(10, truncate=False)
spark.stop()
```

---

## Comandos Makefile Principais

```bash
# Ver comandos dispon√≠veis
make help

# Inicializar projeto completo
make init

# Subir / parar / reiniciar servi√ßos
make up
make down
make restart

# Monitoramento
make status       # Containers ativos
make logs         # Logs dos servi√ßos
make logs-follow  # Logs em tempo real
make health       # Health check

# Limpeza
make clean        # Containers e volumes
make clean-all    # Tudo + imagens Docker
```

# Modelo e Diagrama Entidade-Relacionamento (MER / DER)

Este documento apresenta a estrutura l√≥gica do modelo de dados, ilustrando as entidades (tabelas) e os relacionamentos estabelecidos para integrar os dom√≠nios de Contas Externas (BOP, IIP, IRFCL, ER) e Demografia (DEMOGRAPHY).

---

## 1. Diagrama Entidade-Relacionamento (DER)

Primeiramente desenvolveu-se o DER para representar os dados da maneira como foram extra√≠dos:

<img src="./silver/modelagem/dados/der.png" alt="Diagrama Entidade-Relacionamento" style="max-width: 100%; height: auto;">

Afim de esclarecer a futura estrutura que ser√° utilizada na camada Gold, foi desenvolvido, tamb√©m o DER da futura estrutura do Data Lakehouse:

<img src="./silver/modelagem/schema/der_schema.png" alt="Diagrama Entidade-Relacionamento" style="max-width: 100%; height: auto;">

## 2. Diagrama L√≥gico de Dados (DLD)

Complementarmente ao DER, evolui-se os Diagramas Entidade-Relacionamento para um modelo mais pr√≥ximo do n√≠vel f√≠sico, os chamados Diagramas L√≥gicos de Dados (DLD).

Segue, respectivamente, o DLD da estrutura dos dados e da estrutura que ser√° usada na camada Gold.

Dados:

<img src="./silver/modelagem/dados/dld.png" alt="Diagrama Entidade-Relacionamento" style="max-width: 100%; height: auto;">

Schema:

<img src="./silver/modelagem/schema/dld_schema.png" alt="Diagrama Entidade-Relacionamento" style="max-width: 100%; height: auto;">










